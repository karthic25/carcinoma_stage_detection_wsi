# Adversarial Robustness of Deep Networks in WSI-based Organ Detection Systems

## Introduction
AI systems used in medical imaging are prone to adversarial attacks. Successful attacks can lead to misdiagnosis and misprognosis, leading to increased medical costs for patients and hospitals, reduced Quality-Adjusted Life Years (QALY) for patients, and negatively impact reputation of healthcare institutions. 

In this project, I explore the adversarial robustness of WSI-based organ detection systems to 6 different types of human-undetectable attacks. I have trained the deep neural networks for the detection from scratch, and aim for this analysis to serve as a template for prevention of adversarial attacks in existing systems. I explore 4 state-of-the-art algorithms, namely Resnetv1, Big-Transfer Model (BiT/Resnetv2), CNN, and Vision Transformer (ViT). 

## Design of Experiments
The design of experiments mentioned herein are based on the paper "Ghaffari Laleh, N., Truhn, D., Veldhuizen, G.P. et al. Adversarial attacks and adversarial robustness in computational pathology. Nat Commun 13, 5711 (2022). https://doi.org/10.1038/s41467-022-33266-0". 
