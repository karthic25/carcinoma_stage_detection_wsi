# Adversarial Robustness of Deep Networks in WSI-based Carcinoma Stage Detection Tasks

## Introduction
AI systems used in medical imaging are prone to adversarial attacks. Successful attacks can lead to misdiagnosis and misprognosis, causing increased medical costs for patients and hospitals, reduction in Quality-Adjusted Life Years (QALY) for patients, and reputation damage for healthcare institutions. 

In this project, I explore the adversarial robustness of deep neural networks used in WSI-based carcinoma detection tasks, to 6 types of human-undetectable attacks. By exploring 4 state-of-the-art neural networks namely Resnetv1, Big-Transfer Model (BiT/Resnetv2), CNN, and Vision Transformer (ViT), I aim for this analysis to serve as a template for prevention of adversarial attacks of models in medical imaging diagnosis tasks. 

## Design of Experiments
The design of experiments mentioned herein are based on the paper "Ghaffari Laleh, N., Truhn, D., Veldhuizen, G.P. et al. Adversarial attacks and adversarial robustness in computational pathology. Nat Commun 13, 5711 (2022). https://doi.org/10.1038/s41467-022-33266-0". 
